{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpllA33VGTdY"
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z5JIwchMGP06"
   },
   "outputs": [],
   "source": [
    "## Libraries\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QBKSiGZQJjyg"
   },
   "outputs": [],
   "source": [
    "## Config\n",
    "model_path = 'roneneldan/TinyStories-2Layers-33M'\n",
    "tokenizer_path = \"EleutherAI/gpt-neo-125M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-UVtKl5MTnO"
   },
   "source": [
    "## Familiarization of Code\n",
    "\n",
    "References:\n",
    "1. https://huggingface.co/roneneldan/TinyStories-2Layers-33M\n",
    "2. https://arxiv.org/abs/2305.07759\n",
    "3. https://huggingface.co/blog/how-to-generate\n",
    "4. https://huggingface.co/docs/transformers/generation_strategies#decoding-strategies\n",
    "5. https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4assS1hiGeZR"
   },
   "outputs": [],
   "source": [
    "## Model and Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "def generate_story(prompt):\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch_device)\n",
    "  output = model.generate(input_ids, max_length = 1000, num_beams=1, pad_token_id=tokenizer.eos_token_id)\n",
    "  output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n57yFcVpGfqH",
    "outputId": "99d01251-22eb-48e7-f566-edf11e24776b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl named Lucy. She was three years old and loved to play. One day, Lucy was playing in her room when she heard a loud noise. She looked around and saw a big, scary monster!\n",
      "\n",
      "Lucy was very scared and started to cry. She ran to her mom and dad and told them about the monster. Her mom and dad said, \"Don't worry, Lucy. We will protect you.\"\n",
      "\n",
      "The next day, Lucy and her mom and dad went to the park. They saw the monster again. But this time, the monster was not scary. It was just a big, friendly dog! The dog wagged its tail and licked Lucy's hand.\n",
      "\n",
      "Lucy was so happy that the dog was not scary after all. She played with the dog and they became best friends. From that day on, Lucy and the dog played together every day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time there was\"\n",
    "\n",
    "story = generate_story(prompt)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcwHUkC8JJQ4"
   },
   "source": [
    "## Parameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0KXGbBxV1ld"
   },
   "source": [
    "### Different Decoding Strategies and their implementation in Huggingface Transformers\n",
    "\n",
    "**What is Auto-regressive Language Generation?**\n",
    "\n",
    "Auto-regressive language generation refers to a class of natural language processing (NLP) models that generate sequences of words or tokens one at a time in an autoregressive manner. In this context, \"auto-regressive\" means that the model generates each subsequent word based on the preceding words it has generated. These models are typically probabilistic and are trained to predict the next word in a sequence given the previous context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xnTRTeRQHyPa"
   },
   "outputs": [],
   "source": [
    "## Model and Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "def generate_story(prompt, **kwargs):\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch_device)\n",
    "  output = model.generate(input_ids, **kwargs)\n",
    "  output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J-VYibQZszd"
   },
   "source": [
    "### Readme -> Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zupA6eWvMd4D",
    "outputId": "59ad34fb-ee4e-4036-a007-962191f28b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, Max. He is very friendly and likes to play fetch.\"\n",
      "\n",
      "Lily smiled and said, \"I'm glad you like him. He is very cute and soft. Do you want to pet him?\"\n",
      "\n",
      "Max wagged his tail and licked Lily's face. He was happy to have a new friend.\n",
      "\n",
      "Lily and Max played with the dog for a while. They threw a ball and the dog fetched it. They ran and the dog chased them. They had a lot of fun.\n",
      "\n",
      "But then, Lily's mom called her. It was time to go home. Lily said goodbye to Max and thanked him for playing with her. She said, \"You are a good dog, Max. I had a lot of fun with you. Can we come back and play again?\"\n",
      "\n",
      "Max barked and wagged his tail. He was happy to have a new friend. He liked Lily and her dog. He liked her too.\n",
      "\n",
      "Lily and Max walked home with their moms. They were tired but happy. They had a good day at the park.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy walking with my cute dog\"\n",
    "\n",
    "parameters = {\n",
    "    'max_length' : 1000,\n",
    "    'num_beams' : 1,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY_4BkKIVyX_"
   },
   "source": [
    "## Greedy Search\n",
    "\n",
    "\n",
    "**Greedy search selects from the language model the word with the highest probability as the next word.**\n",
    "\n",
    "![Greedy Search](greedy_search.png)\n",
    "\n",
    "The major drawback of greedy search is that it’s not optimal in producing high-probability sentences, as it misses high probability words hidden behind low probability words.\n",
    "\n",
    "The word \"has\" with its high conditional probability of 0.9 is hidden behind the word \"dog\", which has only the second-highest conditional probability, so that greedy search misses the word sequence \"The\" , \"dog\" , \"has\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 1000,\n",
    "    'num_beams' : 1,\n",
    "    'do_sample' : False,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xN25KwNkQ5Z4",
    "outputId": "ec1b5ee6-217c-473d-cc46-16c07e5ba4ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning about the world around us.\"\n",
      "\n",
      "The little girl smiled and said, \"I'm glad you like it. I'm going to learn more about the world around us.\"\n",
      "\n",
      "The old man smiled and said, \"That's a great idea. I'm going to teach you about the world around us.\"\n",
      "\n",
      "The little girl was so excited. She asked, \"What is it?\"\n",
      "\n",
      "The old man said, \"It's a place where we can learn about the world around us. We can learn about the world and the world around us.\"\n",
      "\n",
      "The little girl was so happy. She thanked the old man and ran off to explore the world. She learned so much about the world and the world around her.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to playing with you again tomorrow!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is three. They are both happy.\n",
      "\n",
      "\"Look, Lily, I have a red car. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster than yours. It is faster. It is faster than yours. It is faster. It is faster than yours. It is faster. It is faster than yours. It is faster. It is faster than yours. It is faster. It is faster than yours. It is faster. It is faster than yours. It is faster. It is faster. It is faster than yours. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster. It is faster.\n",
      "\n",
      "Lily and Ben are not happy. They are angry. They are not friends. They are not friends. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are enemies. They are friends. They are faster. They are friends. They are friends. They are friends. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are friends. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are enemies. They are distant. They are friends. They are distant. They are distant. They are friends. They are enemies. They are distant. They are enemies. They are friends. They are distant. They are friends. They are friends. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are distant. They are enemies. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are distant. They are friends. They are distant. They are distant. They are friends. They are friends. They are distant. They are distant. They are friends. They are distant. They are distant. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are distant. They are distant. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are friends. They are\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vsh01NWxbxtk"
   },
   "source": [
    "## Beam Search\n",
    "\n",
    "\n",
    "**Beam search addresses this problem by keeping the most likely hypotheses (a.k.a. beams) at each time step and eventually choosing the hypothesis that has the overall highest probability.**\n",
    "\n",
    "Let's illustrate with num_beams=2:\n",
    "\n",
    "![Beam Search](beam_search.png)\n",
    "\n",
    "At time step 1, during beam search, the algorithm considers the most likely hypothesis ('The', 'nice') alongside the second most likely one ('The', 'dog'). Moving to time step 2, beam search determines that the word sequence ('The', 'dog', 'has') has a higher probability of 0.36 compared to ('The', 'nice', 'woman') with a probability of 0.2. Thus, the algorithm identifies the most likely word sequence in this simplified example.\n",
    "\n",
    "\n",
    "Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n",
    "\n",
    "</br>\n",
    "Let's see how beam search can be used in transformers. We set num_beams > 1 and early_stopping=True so that generation is finished when all beam hypotheses reached the EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 1000,\n",
    "    'num_beams' : 5,\n",
    "    'do_sample' : False,\n",
    "    'early_stopping' : True,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df87syPRbBgx",
    "outputId": "e8730daa-c74c-4f6b-81c6-de5091781c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning new things.\" \n",
      "\n",
      "Lily smiled and said, \"I love learning new things. It's so much fun!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to playing with you again tomorrow.\"\n",
      "\n",
      "Lily smiled and said, \"I can't wait to play with you again tomorrow!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is four. They are happy.\n",
      "\n",
      "But then, they hear a loud noise. It is a big truck. It is coming fast. It is coming to the park. It is going to hit them.\n",
      "\n",
      "Lily and Ben are scared. They hug each other. They cry.\n",
      "\n",
      "\"Help! Help!\" they shout.\n",
      "\n",
      "A man hears them. He is a policeman. He runs to the park. He sees the truck. He sees Lily and Ben.\n",
      "\n",
      "\"Are you OK?\" he asks.\n",
      "\n",
      "\"Yes, thank you,\" Lily and Ben say.\n",
      "\n",
      "\"Are you OK?\" he asks.\n",
      "\n",
      "\"Yes, thank you,\" Lily and Ben say.\n",
      "\n",
      "\"You're welcome,\" the policeman says.\n",
      "\n",
      "\"You're welcome,\" Lily and Ben say.\n",
      "\n",
      "The policeman smiles. He is not angry. He is kind.\n",
      "\n",
      "\"Are you OK?\" he asks.\n",
      "\n",
      "\"Yes, thank you,\" Lily and Ben say.\n",
      "\n",
      "\"You're welcome,\" the policeman says.\n",
      "\n",
      "\"Have a nice day,\" he says.\n",
      "\n",
      "Lily and Ben wave goodbye to the policeman. They go back to their mom. They tell her what happened. She hugs them.\n",
      "\n",
      "\"I'm glad you're OK,\" she says.\n",
      "\n",
      "\"Me too,\" Lily and Ben say.\n",
      "\n",
      "They go home. They are happy. They are safe. They are friends.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH5aR5IZu2va"
   },
   "source": [
    "## Sampling\n",
    "\n",
    "**Sampling means randomly picking the next word​ according to the conditional word probability distribution extracted from the language model. As a consequence, with this decoding method text generation is not deterministic.**\n",
    "\n",
    "In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution. Taking the example from above, the following graphic visualizes language generation when sampling.\n",
    "\n",
    "![Sampling](sampling1.png)\n",
    "\n",
    "It becomes obvious that language generation using sampling is not deterministic anymore. The word \"car\" is sampled from the conditioned probability distribution P(\"car\" | \"The\"), followed by sampling \"drives\" from P(\"drives\" | \"The\", \"car\").\n",
    "\n",
    "\n",
    "</br>\n",
    "In transformers, we set do_sample=True and deactivate Top-K sampling via top_k=0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 1000,\n",
    "    'do_sample' : True,\n",
    "    'top_k' : 0,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ShHXih3Cd1sc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning and feeling an urge to try new things. Eating a tasty snack is always worth it.\n",
      "Words: use, meal, important\n",
      "Story: \n",
      "\n",
      "Once upon a time, there was a little girl who wanted to learn something new. She had heard about an important feeling the urge from her family that she had heard about. Her family particularly loved and all of her friends liked her food. \n",
      "\n",
      "The little girl asked her mum if she could use the table to eat her meal, for dinner and for teaching her how to use her own meal. Her mum said yes, but she warned the little girl that the table was very special and needed lots of care care about things. \n",
      "\n",
      "So one night, a delicious meal was ready. The little girl used lots of hunger, and it made her dinner very tasty. She ate it all up, and was very happy. \n",
      "\n",
      "After she finished her meal, the little girl smiled. She was glad that her family had taken care of her and used the table in order to eat her meal. It was a special moment that her mum would use her own time to learn new things. \n",
      "\n",
      "And she felt proud of herself for using her own instincts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to this stair\".\n",
      "\n",
      "So his mommy did the most wonderful stair. Dory thought it was a sweet thing for them to walk on. Dory giggled and said, \"Yes! Let's do it!\"\n",
      "\n",
      "So they did. They kept walking slowly and carefully. They slowly began to walk and back down the big stair.\n",
      "\n",
      "Dory had the best time. He had gotten to spend his first stair adventure with his mommy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is while the dinosaurs are slide and shout,\" Your Grandma said, \" absolute unity is all these. Grandma loves us deeply and they believe in us.\"\n",
      "\n",
      "Alice smiled and hugged her Grandma, loving that you really know she should always believe in her Grandma. She thanked her Grandma and promised to always believe anymore, no matter what. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update on scaling with temperature\n",
    "\n",
    "A trick is to make the distribution sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called temperature of the softmax.\n",
    "\n",
    "\n",
    "![sampling_with_temp](samping_with_temp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 1000,\n",
    "    'do_sample' : True,\n",
    "    'top_k' : 0,\n",
    "    'temperature' : 0.6,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning from this.\"\n",
      "\n",
      "After that, Lily went back home and played with her friends. They were happy and had lots of fun together.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to playing with my friends.\" \n",
      "\n",
      "Lily asked, \"What is your secret?\" \n",
      "\n",
      "The elderly man replied, \"I'm an elderly man who delivers presents to children. It's my job to make sure they get good things to bring.\" \n",
      "\n",
      "Lily was curious and asked, \"What did you do with your job?\" \n",
      "\n",
      "The elderly man replied, \"I deliver presents to children all over the world.\" \n",
      "\n",
      "Lily was amazed and said, \"Wow, that's so cool! Can I come with you and play?\" \n",
      "\n",
      "The elderly man smiled and said, \"Of course, you can come with me.\" \n",
      "\n",
      "Lily was so happy to have a new friend and a new adventure. She couldn't wait to see what other surprises the world had in store for her.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is three. They are happy and hug each other. They say, \"We are happy. We have each other. We have our toys. We have our own toys. We have our own friends. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. They are ours. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We are happy. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. They are safe. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. They are ours. We have our own toys. We have our own toys. We have our own toys. We have our own toys. They are ours. We have our own toys. We have our own toys. We have our own toys. We have our own toys. They are ours. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We are happy. We are rich. We have our own toys. They are ours. We have our own toys. They are ours. They are ours. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. We have our own toys. They are ours. They are ours. We have our own toys. They are ours. We have our own toys. They are ours. We are rich. We have our own toys. We have each other. We have our own toys. We have our own toys. We have our own toys. They are ours. We are happy. We are twins. We are rich.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "\n",
    "\n",
    "In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words.\n",
    "\n",
    "We extend the range of words used for both sampling steps in the example above from 3 words to 10 words to better illustrate Top-K sampling.\n",
    "\n",
    "![Top-K Sampling](top_k_sampling.png)\n",
    "\n",
    "Having set K=6, in both sampling steps we limit our sampling pool to 6 words. While the 6 most likely words, defined as Vtop-K ​ encompass only ca. two-thirds of the whole probability mass in the first step, it includes almost all of the probability mass in the second step. Nevertheless, we see that it successfully eliminates the rather weird candidates (“not\",“the\",“small\",“told\") in the second sampling step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 1000,\n",
    "    'do_sample' : True,\n",
    "    'top_k' : 10,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning.\" \n",
      "\n",
      "After the haircut, Timmy looked in the mirror and saw that he looked very handsome. \"Wow, I look great!\" he exclaimed. \n",
      "\n",
      "Mommy smiled and said, \"Yes, you do! You look very nice in your new haircut.\" Timmy was happy and said, \"Thank you!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to taking a nap with my new toy,\" replied Lily.\n",
      "\n",
      "Timmy was curious and asked, \"What's a nap?\"\n",
      "\n",
      "Lily explained, \"A nap is when you close your eyes and rest your body to feel better. But first, let's clean up the toys and put them away.\"\n",
      "\n",
      "Timmy understood and they cleaned up their toys. From that day on, they became good friends and played together every day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is three. Tom is four. Six.\n",
      "\n",
      "\"What is the name of your house?\" Lily asks.\n",
      "\n",
      "\"It is the name of our house,\" Ben says.\n",
      "\n",
      "\"What is the name of our house?\" Lily asks.\n",
      "\n",
      "Tom thinks for a moment. He looks at the house. It is old and white. He has an idea.\n",
      "\n",
      "\"It is the name of our house,\" he says. \"The person who is wealthy lives in their house. They have a lot of things and food and toys. They also has a big house and a pool and a lot of things.\"\n",
      "\n",
      "Lily and Ben are curious. They want to know more about their house. They nod their heads.\n",
      "\n",
      "\"That is good,\" Ben says. \"The wealthy person who is wealthy and has many things. Maybe he will have a house and a house and a lot of things.\"\n",
      "\n",
      "Lily and Ben agree. They think that is nice. They think that is a good idea. They decide to ask the wealthy person who lives in the house next to them. They are happy and curious and want to learn more about their home.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P Sampling\n",
    "\n",
    "Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. \n",
    "\n",
    "\n",
    "![Top-P Sampling](top_p_sampling.png)\n",
    "\n",
    "Having set p=0.92, Top-p sampling picks the minimum number of words to exceed together p=92% of the probability mass, defined as Vtop-p ​ . In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. \n",
    "\n",
    "Quite simple actually! It can be seen that it keeps a wide range of words where the next word is arguably less predictable, e.g. P(w∣\"The”), and only a few words when the next word seems more predictable, e.g. P(w∣\"The\",\"car\").\n",
    "\n",
    "\n",
    "We activate Top-p sampling by setting 0 < top_p < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 1000,\n",
    "    'do_sample' : True,\n",
    "    'top_k' : 0,\n",
    "    'top_p' : 0.92,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning,\" replied his mom.\n",
      "\n",
      "As they were finishing up, Timmy's dad came home from work. \"Wow, Timmy! You're getting so good at math,\" he said. \"Yeah, and we love to learn new things.\" Timmy smiled and felt proud of himself for learning something new.\n",
      "\n",
      "Later that day, Timmy went to play with his friend Johnny. \"Look at this beautiful patch of flowers I found!\" Timmy said excitedly. \"That's amazing, Timmy! You must be really good at math,\" Johnny replied. Timmy felt even more proud and happy that he could learn something new. From that day on, Timmy continued to learn and practice math, and he always remembered the time he learned something new.\n",
      "Summary: Timmy loves to learn new things and helps his mom prepare for his math test by matching all the pieces of flowers she finds. He continues to learn and feels proud of himself.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to getting bigger and bigger until it's as big as you go!\" \n",
      "\n",
      "The couple's eyes widened in surprise. They realized that the fog was actually helping them get bigger and bigger! The couple had finally gone on an adventure and ended up safe from the icy ground.\n",
      "\n",
      "The moral of the story is that sometimes things that seem scary can turn out to be helpful. Sometimes, things that seem too scary can actually be good. It's important to try new things and understand the difficulties of things.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is 16.\"\n",
      "\n",
      "The pig was so excited and he ran outside to show his friends. Everyone was impressed, so they all clapped. The pig was very proud. He wanted to repeat something more, so he ran back inside to find his mom.\n",
      "\n",
      "When he returned, he asked his mom if he could repeat and check her directions to find his mom. She said \"I'm just looking, I found my friend. His name is Noah.\" \n",
      "\n",
      "The pig was overjoyed. He could not wait to tell his mom and him all the adventures that had happened. He quickly agreed, but promised to be more careful next time. \n",
      "\n",
      "The next day, everyone said the same word to Noah. He remembered what happened yesterday and he felt really proud of himself for remembering - that he had been ignorant!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Top-P & Top-K\n",
    "\n",
    "While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_new_tokens' : 100,\n",
    "    'do_sample' : True,\n",
    "    'top_k' : 10,\n",
    "    'top_p' : 0.92,\n",
    "    'pad_token_id' : tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy deep learning and exploring new things!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I enjoy deep learning\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I look forward to the next time I can help you!\"\n",
      "Summary: A little girl helps a big tree grow taller by giving it water and a hug, and is excited to see the sun setting in the west.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two plus two is the same number and three. They are very good at numbers. They can make numbers bigger and longer.\n",
      "\n",
      "\"Look, I have a number two!\" Tom says.\n",
      "\n",
      "\"Look, I have a number two!\" Mia says.\n",
      "\n",
      "They count their blocks. They have ten. They have ten blocks.\n",
      "\n",
      "\"What are these?\" Tom asks.\n",
      "\n",
      "\"I have a number. It is eleven.\" Mia asks.\n",
      "\n",
      "\"I have a number. It is\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Two plus two is\"\n",
    "story = generate_story(prompt, **parameters)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradeoff Observations\n",
    "\n",
    "\n",
    "Here, we can discuss the model's response to a basic and broad prompt, 'I enjoy deep learning,' which would not be specifically covered in its training. The model exhibited varying behaviors under different sampling settings:\n",
    "\n",
    "1. Low Consistency and Low Creativity: When used beam search and sampling with temperature, it tend to produce output which were small, less creative and not consistent at all. \n",
    "> Output: <br>I enjoy deep learning from this.<br>After that, Lily went back home and played with her friends. They were happy and had lots of fun together.\n",
    "\n",
    "2. Low Consistency and High Creativity: When temperature was set at 1.0 and either only higher top p around 0.92 or top k around 10 was used, the results produced were creative, grammatically correct but made no sense. There was no coherence at all.\n",
    "> Output (top_p = 0.92):<br> \"I enjoy deep learning,\" replied his mom.<br> As they were finishing up, Timmy's dad came home from work. \"Wow, Timmy! You're getting so good at math,\" he said. \"Yeah, and we love to learn new things.\" Timmy smiled and felt proud of himself for learning something new.<br><br>As they were finishing up, Timmy's dad came home from work. \"Wow, Timmy! You're getting so good at math,\" he said. \"Yeah, and we love to learn new things.\" Timmy smiled and felt proud of himself for learning something new.<br><br>Later that day, Timmy went to play with his friend Johnny. \"Look at this beautiful patch of flowers I found!\" Timmy said excitedly. \"That's amazing, Timmy! You must be really good at math,\" Johnny replied. Timmy felt even more proud and happy that he could learn something new. From that day on, Timmy continued to learn and practice math, and he always remembered the time he learned something new.\n",
    "Summary: Timmy loves to learn new things and helps his mom prepare for his math test by matching all the pieces of flowers she finds. He continues to learn and feels proud of himself.\"\n",
    "\n",
    "> Output (top_k = 10):<br>I enjoy deep learning. <br><br>After the haircut, Timmy looked in the mirror and saw that he looked very handsome. \"Wow, I look great!\" he exclaimed. <br><br>Mommy smiled and said, \"Yes, you do! You look very nice in your new haircut.\" Timmy was happy and said, \"Thank you!\"\n",
    "\n",
    "3. Balanced Approach: A moderate setting of temperature at 1.0, top_k at 10, top_p at 0.92 yielded a balance, producing text that made sense and maintained coherence. \n",
    "> Output: \"I enjoy deep learning and exploring new things!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As ad-hoc decoding methods, top-p and top-K sampling seem to produce more fluent text than traditional greedy - and beam search on open-ended language generation. There is evidence that the apparent flaws of greedy and beam search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method\n",
    "\n",
    "The behavior of language models like the roneneldan/TinyStories-2Layers-33M, in response to different prompts can be attributed to several aspects of their design and training. Understanding why a model might respond to a prompt like \"Two plus two is\" with a creative continuation rather than a factual answer may be due to following reasons:\n",
    "\n",
    "1. Training Data and Objectives\n",
    "The TinyStories dataset focuses on short stories, which means the model is trained primarily on narrative text for 3-4 years old. This training influences the model to favor creative storytelling. When faced with a prompt, the model draws from this narrative style, leading to imaginative continuations rather than literal or factual responses.\n",
    "\n",
    "2. The dataset's limited vocabulary also shapes the model's responses. It might not have extensive exposure to mathematical or factual statements, skewing its responses towards storytelling.\n",
    "\n",
    "3. Contextual Understanding\n",
    "Lack of Real-World Knowledge: Unlike humans, language models don't have inherent real-world knowledge or understanding. Their \"knowledge\" is limited to patterns learned from the training data. When presented with a prompt like \"Two plus two is,\" the model doesn't understand the factual basis of the statement. However, it bases its generation on the statistical likelihood of sequences in the training data rather than logical or factual accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
